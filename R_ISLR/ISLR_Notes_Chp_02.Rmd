---
title: "ISLR Notes and Exercises"
author: "Nathaniel Lai"
date: "Updated March 2018"
output:
  html_document:
    keep_md: false
    keep_tex: false
  pdf_document:
    toc: no
    toc_depth: 2
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 2: Statistical Learning

## Bias-Variance Tradeoff

One of the key concepts in statistical learning is the bias-variance tradeoff (diagram from [Weatherwax](http://waxworksmath.com/Authors/G_M/James/james.html)).

* `red` = test error 
* `orange` = estimator variance 
* `green` = model bias 
* `gray` = irreducible error 
* `blue` = training error \vspace{0.3 cm}

```{r, echo=FALSE, fig.height = 4.7, fig.width = 6.5}
curve(82*x, from=0, to=11, xlab="Flexibility", ylab="MSE", col="white") 
curve(300*cos(x/3)+500+x^3/3, add=TRUE, col="red", lwd=2)  # test error
curve(x^3/3, add=TRUE, col="orange", lwd=2)                # variance
curve(0*x+250, add=TRUE, col="gray", lwd=2)                # irreducible error
curve(300*cos(x/3)+350,  from=0, to=9, add=TRUE, col="green", lwd=2)      # bias
curve(300*cos(3)+350 + (0*x),  from=9, to=11, add=TRUE, col="green", lwd=2)
curve(225*cos(x/3)+450, from=0, to=9, add=TRUE, col="blue", lwd=2)       # train error
curve(225*cos(3)+450 + (0*x),  from=9, to=11, add=TRUE, col="blue", lwd=2)
```

where
$$
  MSE= \frac{1}{n} \sum{(y_i-\hat{f}(x_i)^2}
$$
(for regression models) and, for classification, the training error rate is :
$$
  \frac{1}{n} \sum^n_{i=1} I(y_i\ne\hat{y_i})
$$

The bias-variance tradeoff decomposes the *expected test MSE* into:
\vspace{0.2 cm}
$$
E(y_0-\hat{f}(x_0))^2 = Var(\hat{f}(x_0) + [Bias(\hat{f}(x_0))]^2 + Var(\varepsilon)
$$
\vspace{0.3 cm}
$$
\textit{Expected MSE} = \underbrace{variance + bias}_\textit{reducible error} + \textit{irreducible error}
$$

"When a given method yields a small training MSE but a large test MSE, we are said to be \textbf{overfitting} the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function `f`. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply do not exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE." (ISLR P.32)
 
It is possible to show that the test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. In other words, we should simply assign a test observation with predictor vector $x_0$ to the class $j$ for which the \textbf{conditional probability} (ISLR P.38)
$$
  Pr(Y = j | X = x_0)
$$
is the largest with the error rate:
$$
  1 - E \Big(\text{max}_{j} Pr(Y = j | X \Big)
$$

## Prediction vs. Model Accuracy

### Question 5

Advantages of a very flexible model include better fit to data and fewer prior assumptions. 

Disadvantages are the increased difficulties to interpret and the danger of overfitting. 

A more flexible approach might be preferred is the underlying data is very complex (simple linear fit does not suffice) or if we mainly care about the result and not inference, provided that sample size is large enough. 

A less flexible model is preferred is the underlying data has a simple shape or if inference and interpretability are important.


## Parametric and Non-parametric Methods 

### Question 6

For parametric methods, we make an assumption about the shape of the underlying data, select a model form, and fit the data to our selected form. The advantage is that we can incorporate any prior/expert knowledge and do not tend to have too many parameters that need to be fit. To the extent that our prior/expert assumptions are wrong, then that would be a disadvantage.

Non-parametric methods do not make explicit assumptions on the shape of the data, which could be an advantage. The key disadvantage is that they need a large number of observations to fit an accurate estimate.


### Applied Questions

### Question 8

This question is on standard regrassion prcedures and the use of `ggplot`.

__Part a)__

```{r}
require(ISLR);
data(College)
str(College)
```

__Part b)__

```{r, eval = FALSE}
# these steps were already taken on College data in the ISLR package
rownames(College) <- College[,1]  # set row names
College <- College[,-1]  # drop first col
# i.
summary(College)
```

__Part c)__

```{r}
attach(College)
# ii.
pairs(College[,1:10])
# iii.
```
```{r fig.height = 4, fig.width = 5, fig.align='center'}
boxplot(Outstate~Private, data=College, xlab="Private", ylab="Outstate")
```
```{r fig.height = 4, fig.width = 5, fig.align='center'}
# iv.
Elite <- rep("No", nrow(College))
Elite[Top10perc>50] <- "Yes"
College <- data.frame(College, Elite)
summary(College)  # 78 Elite
boxplot(Outstate~Elite, data=College, xlab="Elite", ylab="Outstate")
```
```{r fig.height = 4, fig.width=7, fig.align='center'}
# v. 
par(mfrow=c(2,2))
hist(Apps, breaks=50, xlim=c(0,25000), main="Apps")
hist(Enroll, breaks=25, main="Enroll")
hist(Expend, breaks=25, main="Expend")
hist(Outstate, main="Outstate")
```
```{r echo = FALSE, fig.height = 4, fig.width = 7, fig.align='center'}
library(ggplot2)
par(mfrow=c(1,1))
ggplot(data = College) + 
  geom_point(mapping = aes(x = Outstate, y = Grad.Rate, color = Private)) + 
  geom_smooth(mapping = aes(x = Outstate, y = Grad.Rate))

# Check geom_ribbon(aes())
# plot(Outstate, Grad.Rate)
# High tuition correlates to high graduation rate. And Private College correlates with 
# higher grad.rate and out of state tuition 
```

```{r echo = FALSE, fig.height = 4, fig.width = 7, fig.align='center'}
ggplot(data = College, mapping = aes(x = Accept/Apps, y = S.F.Ratio, color = Elite, alpha= 0.8)) + 
  geom_point()

#plot(Accept / Apps, S.F.Ratio)
# Colleges with low acceptance rate tend to have low Student-to-faculty ratio.
```

```{r echo = FALSE,fig.height = 4, fig.width = 6, fig.align='center'}
ggplot(data = College) + 
  geom_point(mapping = aes(x=Top10perc, y=Grad.Rate, color = Private, alpha= 0.8)) +
  geom_jitter(mapping = aes(x=Top10perc, y=Grad.Rate, color = Private), width = 0.6, height = 0.6)
```
Colleges with the most students from top $10\%$ perc do not necessarily have the highest graduation rate. Also, rate over 100 is erroneous. 

